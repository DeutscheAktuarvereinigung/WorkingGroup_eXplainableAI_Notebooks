{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Simon-Style -->\n",
    "<p style=\"font-size:19px; text-align:left; margin-top:    15px;\"><i>German Association of Actuaries (DAV) — Working Group \"Explainable Artificial Intelligence\"</i></p>\n",
    "<p style=\"font-size:25px; text-align:left; margin-bottom: 15px\"><b>Use Case SOA GLTD Experience Study:<br>\n",
    "Data initialisation\n",
    "</b></p>\n",
    "<p style=\"font-size:19px; text-align:left; margin-bottom: 15px; margin-bottom: 15px\">Guido Grützner (<a href=\"mailto:guido.gruetzner@quantakt.com\">guido.gruetzner@quantakt.com</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This notebook reads the SOA GLTD experience study data in the original zipped format (zipped csv) as found in \"https://cdn-files.soa.org/2019-group-ltd-exp-studies/2009-2013-gltd-consolidated-database.zip\" and transforms the file into several smaller files in feather format.\n",
    "\n",
    "The path to your local copy of the zip-file has to be assigned to the variable `datadir` in line 8 of the first code block below.\n",
    "\n",
    "The number of resulting files after the split is determined by the variable `anzblock` defined below. The current number is sufficient large and the resulting files sufficiently small to enable reading one(!) of those files into a PC with 8GB of RAM. The script itself should run on a 8GB RAM Laptop in roughly twenty minutes. It only needs to run once. The total size of all .feather files is roughly 2GB.\n",
    "\n",
    "Each of the resulting split files contains a random sample (without replacement) of the total database. The split files are disjoint, and the union of all split files is the total database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-20T15:53:50.335751Z",
     "iopub.status.busy": "2024-07-20T15:53:50.335751Z",
     "iopub.status.idle": "2024-07-20T15:53:50.656885Z",
     "shell.execute_reply": "2024-07-20T15:53:50.656885Z",
     "shell.execute_reply.started": "2024-07-20T15:53:50.335751Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 47110815\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Adjust according to your local setup\n",
    "# LOCATION OF GLTD DATA\n",
    "datadir = \"d:/tmp/GLTD data/\"\n",
    "fn_in = \"2009-2013-gltd-consolidated-database.zip\"\n",
    "\n",
    "\n",
    "incols = ['Study_ID', 'Elimination_Period', 'Calendar_Year', 'Calendar_Month',\n",
    "          'Duration_Month', 'Age_at_Disability', 'Diagnosis_Category',\n",
    "          'OwnOccToAnyTransition', 'Gender', 'Attained_Age',\n",
    "          'Mental_and_Nervous_Period', 'M_N_Limit_Transition',\n",
    "          'Gross_Indexed_Benefit_Amount', 'Industry', 'Indexed_Monthly_Salary',\n",
    "          'Taxability_of_Benefits', 'Integration_with_STD', 'Case_Size',\n",
    "          'Residence_State', 'COLA_Indicator', 'Benefit_Max_Limit_Proxy',\n",
    "          'Replacement_Ratio', 'Original_Social_Security_Award_Status',\n",
    "          'Updated_Social_Security_Award_Status', 'Exposures',\n",
    "          'Actual_Recoveries', 'Actual_Deaths', 'Settlement_Counts',\n",
    "          'Max_Out_Counts', 'Limits_Count']\n",
    "\n",
    "# Please, don't change the random seed when working in a team\n",
    "# You will select base data different from everyone else!\n",
    "sq = np.random.SeedSequence()\n",
    "print('seed = {}'.format(47110815))\n",
    "rng = np.random.default_rng(seed=sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups are created according to Study_IDs. Study_IDs are assigned randomly to groups, but it is assured that all records (=lines of the csv) with the same `Study_ID` end up in the same group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T15:53:50.656885Z",
     "iopub.status.busy": "2024-07-20T15:53:50.656885Z",
     "iopub.status.idle": "2024-07-20T15:55:41.830509Z",
     "shell.execute_reply": "2024-07-20T15:55:41.830509Z",
     "shell.execute_reply.started": "2024-07-20T15:53:50.656885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine groups\n",
    "rawtbl = pd.read_table(datadir + fn_in, usecols=[\"Study_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T15:55:41.830509Z",
     "iopub.status.busy": "2024-07-20T15:55:41.830509Z",
     "iopub.status.idle": "2024-07-20T15:55:42.489798Z",
     "shell.execute_reply": "2024-07-20T15:55:42.489798Z",
     "shell.execute_reply.started": "2024-07-20T15:55:41.830509Z"
    }
   },
   "outputs": [],
   "source": [
    "id_uq = pd.Series(rawtbl[\"Study_ID\"].unique())\n",
    "n = id_uq.size\n",
    "# apply a random permutation to the sequence of IDs\n",
    "id_uq = id_uq.sample(n=n, random_state=rng, replace=False)\n",
    "# this defines the number of blocks (i.e. anzblock + 1)\n",
    "anzblock = 4  # should be OK for 8GB RAM\n",
    "# unique IDs per block\n",
    "nid_bk = np.floor(n * 0.9 / anzblock).astype(int)\n",
    "# Rest is UHM data which should be kept separate\n",
    "nid_uhm = (n - anzblock * nid_bk).astype(int)\n",
    "# names of output files, extension include the \".\"\n",
    "fn_ext = \".gz\"\n",
    "nm_out = [\"gltd09_13_pt\" + str(i) + fn_ext for i in range(anzblock)]\n",
    "nm_out.append(\"uhmgltd09_13\" + fn_ext)\n",
    "\n",
    "# create groups mapping\n",
    "tt = np.array([np.repeat(igrp, nid_bk) for igrp in range(anzblock)]).ravel()\n",
    "grp = pd.Series(np.concatenate((tt, np.repeat(anzblock, nid_uhm))))\n",
    "id2grp = pd.DataFrame({\"id_uq\": id_uq, \"grp\": grp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T15:55:42.489798Z",
     "iopub.status.busy": "2024-07-20T15:55:42.489798Z",
     "iopub.status.idle": "2024-07-20T16:06:10.876913Z",
     "shell.execute_reply": "2024-07-20T16:06:10.876913Z",
     "shell.execute_reply.started": "2024-07-20T15:55:42.489798Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.read_table(datadir + fn_in, usecols=incols, engine=\"c\",\n",
    "                   chunksize=200000) as reader:\n",
    "    for chunk in reader:\n",
    "        tt = chunk.merge(id2grp, left_on='Study_ID', right_on='id_uq')\n",
    "        tt.drop([\"id_uq\"], axis=1, inplace=True)\n",
    "        grouped = tt.groupby([\"grp\"])\n",
    "\n",
    "        for fnum, group in grouped:\n",
    "            fn = datadir + nm_out[fnum[0]]\n",
    "            flg_header = not os.path.isfile(fn)\n",
    "            group.drop([\"grp\"], axis=1, inplace=True)\n",
    "            group.to_csv(fn, sep=\"\\t\", header=flg_header, mode=\"a\",\n",
    "                         index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:06:10.876913Z",
     "iopub.status.busy": "2024-07-20T16:06:10.876913Z",
     "iopub.status.idle": "2024-07-20T16:08:14.929860Z",
     "shell.execute_reply": "2024-07-20T16:08:14.929860Z",
     "shell.execute_reply.started": "2024-07-20T16:06:10.876913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "nm_feather = [nm[:-len(fn_ext) + 1] + \"feather\" for nm in nm_out]\n",
    "for ifile in range(len(nm_out)):\n",
    "    fn_csv = datadir + nm_out[ifile]\n",
    "    fn_feather = datadir + nm_feather[ifile]\n",
    "    if os.path.isfile(fn_csv):\n",
    "        print(ifile)\n",
    "        tt = pd.read_csv(fn_csv, sep=\"\\t\")\n",
    "        tt.to_feather(fn_feather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertrauen ist gut Kontrolle ist besser!\n",
    "\n",
    "These Cells need to be run only for validation purposes. Once, after changes to the code above were made. Note that you need a machine with sufficient RAM ($\\geq$32GB) to run these validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:08:14.929860Z",
     "iopub.status.busy": "2024-07-20T16:08:14.929860Z",
     "iopub.status.idle": "2024-07-20T16:08:14.936344Z",
     "shell.execute_reply": "2024-07-20T16:08:14.936344Z",
     "shell.execute_reply.started": "2024-07-20T16:08:14.929860Z"
    }
   },
   "outputs": [],
   "source": [
    "# # repeat assignments to make this run stand-alone\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# pd.options.mode.copy_on_write = True\n",
    "\n",
    "# datadir = \"d:/tmp/GLTD data/\"\n",
    "# fn_in = \"2009-2013-gltd-consolidated-database.zip\"\n",
    "# rawtbl = pd.read_table(datadir + fn_in)\n",
    "\n",
    "# anzblock = 4\n",
    "# fn_ext = \".gz\"\n",
    "# nm_out = [\"gltd09_13_pt\" + str(i) + fn_ext for i in range(anzblock)]\n",
    "# nm_out.append(\"uhmgltd09_13\" + fn_ext)\n",
    "# nm_feather = [nm[:-len(fn_ext) + 1] + \"feather\" for nm in nm_out]\n",
    "# tt = [pd.read_feather(datadir + ifile) for ifile in nm_feather]\n",
    "# df_in = pd.concat(tt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:08:14.936344Z",
     "iopub.status.busy": "2024-07-20T16:08:14.936344Z",
     "iopub.status.idle": "2024-07-20T16:08:14.942178Z",
     "shell.execute_reply": "2024-07-20T16:08:14.942178Z",
     "shell.execute_reply.started": "2024-07-20T16:08:14.936344Z"
    }
   },
   "outputs": [],
   "source": [
    "# iduq_raw = pd.unique(rawtbl[\"Study_ID\"])\n",
    "# iduq_df = pd.unique(df_in[\"Study_ID\"])\n",
    "# all(iduq_raw == iduq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:08:14.942178Z",
     "iopub.status.busy": "2024-07-20T16:08:14.942178Z",
     "iopub.status.idle": "2024-07-20T16:08:14.947551Z",
     "shell.execute_reply": "2024-07-20T16:08:14.947551Z",
     "shell.execute_reply.started": "2024-07-20T16:08:14.942178Z"
    }
   },
   "outputs": [],
   "source": [
    "# # get records per Study_ID\n",
    "# rawrecperid = rawtbl[\"Study_ID\"].value_counts()\n",
    "# dfrecperid = df_in[\"Study_ID\"].value_counts()\n",
    "# all(rawrecperid == dfrecperid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T16:08:14.947551Z",
     "iopub.status.busy": "2024-07-20T16:08:14.947551Z",
     "iopub.status.idle": "2024-07-20T16:08:14.952641Z",
     "shell.execute_reply": "2024-07-20T16:08:14.952641Z",
     "shell.execute_reply.started": "2024-07-20T16:08:14.947551Z"
    }
   },
   "outputs": [],
   "source": [
    "# abs(rawtbl.Exposures.sum() - df_in.Exposures.sum()) < 1e-7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
